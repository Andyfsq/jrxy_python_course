{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 金融领域中的自然语言处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP本身是人工智能中的一个重要的方向，简单来说，处理自然语言的过程就是让机器去理解人的文本或语言，其中如翻译、语音识别、语义理解、智能问答，知识图谱等都属于NLP的范畴。\n",
    "\n",
    "自计算机诞生伊始，人类就致力于让机器来理解我们语言。随着人工智能、计算机科学、信息工程、统计学、甚至语言学等学科知识的不断进步，目前NLP已经拥有了大量的商业应用，如机器翻译（Google翻译、有道翻译等）、知识图谱（以Google为代表的搜索引擎）、智能问答（Apple的Siri、亚马逊的Alexa以及各种智能机器人）等等。\n",
    "\n",
    "但是，金融领域的NLP目前仍处于探索阶段，金融本身是一个专业性很高的领域，很多词汇在金融语境下会产生特殊含义，所有的子问题都会有一个独特的理解方式，而且金融领域衡量处理结果的方式也与其他领域不同。比如针对舆情分析，金融领域要求对市场未来的走势有一定的预见性。\n",
    "\n",
    "因此，金融领域的NLP需要准备特殊的训练数据集，而目前NLP所有方法都是基于大量的数据集基础上，数据集的缺乏也是目前NLP在金融领域所面临的最大问题之一，这也是金融领域高度的专业性与深度导致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一个强大的NLP系统能够帮助金融机构解决哪些实际问题？\n",
    "\n",
    "全网舆情监控、产业链分析、让机器帮助金融机构阅读大量新闻。\n",
    "\n",
    "例如，商业银行希望使用更全面的数据进行企业的信贷风险管理，提前感知企业的潜在风险。目前常规的风险评估方法是根据企业公布的年报，并综合信贷员实地调查的结果进行判断，但是由于企业自身风险报出通常具有滞后性，公开信息覆盖度不高，看到的往往只是冰山一角，因此判断风险的手段十分单一。这也是NLP与人工智能可以发挥作用的地方。\n",
    "\n",
    "NLP可以对信息进行多维关系的挖掘，评估企业之间的关系，并通过知识图谱直观呈现企业之间的关联，提前设立预警信号，一旦企业关系网内的相关对象出现任意变动，便可根据关系权重，快速地评估对整个关系网的影响程度。\n",
    "\n",
    "![](http://5b0988e595225.cdn.sohucs.com/images/20180817/1be14f4f13914a80bd3c29ed7e74b4c4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 金融语义应用场景概念框\n",
    "\n",
    "1. 智能问答和语义搜索\n",
    "\n",
    "智能问答和语义搜索是自然语言处理（NLP）的关键技术，目的是让用户以自然语言形式提出问题，深入进行语义分析，以更好理解用户意图，快速准确获取知识库中的信息。在用户界面上，既可以表现为问答机器人的形式（智能问答），也可以为搜索引擎的形式（语义搜索）。智能问答系统一般包括问句理解、信息检索、答案生成三个环节。基于知识图谱的智能问答相比基于文本的问答更能满足金融业务实际需求。\n",
    "\n",
    "2. 资讯与舆情分析\n",
    "\n",
    "金融资讯信息非常丰富，例如公司新闻（公告、重要事件、财务状况等）、金融产品资料（股票、证券等）、宏观经济（通货膨胀、失业率等）、政策法规（宏观政策、税收政策等）、社交媒体评论等。\n",
    "\n",
    "3. 金融预测和分析\n",
    "\n",
    "基于语义的金融预测即利用金融文本中包含的信息预测各种金融市场波动，它是以NLP等人工智能技术与量化金融技术的结合。\n",
    "\n",
    "4. 文档信息抽取\n",
    "\n",
    "信息抽取是NLP的一种基础技术，是NLP进一步进行数据挖掘分析的基础，也是知识图谱中知识抽取的基础。采用的方法包括基于规则模板的槽填充的方法、基于机器学习或深度学习的方法。按抽取内容分可以分为实体抽取、属性抽取、关系抽取、规则抽取、事件抽取等。\n",
    "\n",
    "5. 自动文档生成\n",
    "\n",
    "自动文档生成指根据一定的数据来源自动产生各类金融文档。常见的需要生成的金融文档如信息披露公告（债券评级、股转书等）、各种研究报告。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LAC  分词 \n",
    "\n",
    "LAC全称Lexical Analysis of Chinese，是百度自然语言处理部研发的一款联合的词法分析工具，实现中文分词、词性标注、专名识别等功能。\n",
    "\n",
    "代码兼容Python2/3\n",
    "\n",
    "- 全自动安装: ``pip install lac``\n",
    "- 使用百度源安装，安装速率更快：``pip install lac -i https://mirror.baidu.com/pypi/simple``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['LAC', '是', '个', '优秀', '的', '分词', '工具'], ['百度', '是', '一家', '高科技', '公司']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAC import LAC\n",
    "\n",
    "# 装载分词模型\n",
    "lac = LAC(mode='seg')\n",
    "\n",
    "# 单个样本输入\n",
    "text = u\"LAC是个优秀的分词工具\"\n",
    "seg_result = lac.run(text)\n",
    "\n",
    "# 批量样本输入, 输入为多个句子组成的list，平均速率会更快\n",
    "texts = [u\"LAC是个优秀的分词工具\", u\"百度是一家高科技公司\"]\n",
    "seg_result = lac.run(texts)\n",
    "\n",
    "seg_result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结巴分词\n",
    "\n",
    "jieba是一个Python 中文分词组件，参见https://github.com/fxsjy/jieba  \n",
    "\n",
    "可以对中文文本进行分词、词性标注、关键词抽取等功能，并且支持自定义词典。\n",
    "\n",
    "可以直接使用pip来进行安装：\n",
    "``pip install jieba``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "text = '我来到北京清华大学'\n",
    "    \n",
    "seg_list = jieba.cut(text, cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pair('我', 'r'), pair('来到', 'v'), pair('北京', 'ns'), pair('清华大学', 'nt')]\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as posseg\n",
    "seg = posseg.lcut(text)\n",
    "print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键词提取\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/11482169-b1f9fd2c1fc2bffe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TF-IDF 算法的关键词提取\n",
    "\n",
    "  TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)是一种统计方法，用以评估一个词语对于一个文件集或一个语料库中的一份文件的重要程度，其原理可概括为：\n",
    "\n",
    "> 一个词语在一篇文章中出现次数越多，同时在所有文档中出现次数越少，越能够代表该词语很关键\n",
    "\n",
    "计算公式：TF-IDF = TF * IDF，其中：\n",
    "\n",
    "- TF(term frequency, TF)：词频，某一个给定的词语在该文件中出现的次数\n",
    "\n",
    "- IDF(inverse document frequency, IDF)：逆文件频率，如果包含词条的文件越少，则说明词条具有很好的类别区分能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "反转 0.41\n",
      "A股 0.41\n",
      "50% 0.41\n",
      "起点 0.39\n",
      "季度 0.33\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from jieba.analyse import *\n",
    "import jieba.analyse as analyse\n",
    "\n",
    "with open('sample.txt', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print('TF-IDF')\n",
    "for keyword, weight in analyse.extract_tags(data, withWeight=True, topK=5):\n",
    "    print('{} {}'.format(keyword, np.round(weight,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TextRank 算法的关键词提取\n",
    "\n",
    "TextRank 是另一种关键词提取算法，基于大名鼎鼎的 PageRank，其原理可参见论文—— [TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textbank\n",
      "银行 1.0\n",
      "起点 0.92\n",
      "反转 0.85\n",
      "处于 0.81\n",
      "季度 0.49\n"
     ]
    }
   ],
   "source": [
    "print('Textbank')\n",
    "for keyword, weight in analyse.textrank(data, withWeight=True, topK=5):\n",
    "    print('{} {}'.format(keyword, np.round(weight,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生物 1.0\n",
      "提出 0.82\n",
      "投资人 0.78\n",
      "公司 0.59\n",
      "管理层 0.57\n",
      "相信 0.52\n",
      "感情 0.52\n",
      "应该 0.44\n",
      "产品 0.4\n",
      "上市 0.39\n",
      "情感评分（0.6以上为积极，0.2一下为负面）： 0.09\n"
     ]
    }
   ],
   "source": [
    "from snownlp import SnowNLP # 使用\n",
    "from snownlp import seg  # 分词库\n",
    "from snownlp import sentiment # 情感分词\n",
    "from snownlp import normal #停用词处理\n",
    "import numpy as np\n",
    "\n",
    "text = '12月5日14时30分，在“沃森生物转让泽润生物股权”的电话会上，沃森生物董事长李云春遭投资人猛烈炮轰。除了质疑贱卖子公司，投资人还提出公司应该停牌，甚至提出向监管层举报。“你们把我们这些炒股票的当傻子吗？你看看万泰生物值多少钱，你竟然卖的那么低！你们这些人不相信因果报应吗？”“你们是主动卖泽润的还是泽润管理层逼迫你们卖的？”“泽润产品马上上市了，可以自己造血了，为什么要卖？”对此，公司管理层的回答则是——“我们主动卖的，我们是专业的，我们是对沃森倾注了感情的，请相信我们”。'\n",
    "\n",
    "for keyword, weight in analyse.textrank(text, withWeight=True, topK=10):\n",
    "    print('{} {}'.format(keyword, np.round(weight,2)))\n",
    "s = SnowNLP(text)\n",
    "print(\"情感评分（0.6以上为积极，0.2一下为负面）：\",np.round(s.sentiments,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2924920471019137,\n",
       " 0.8717508639593845,\n",
       " 0.9933946532455206,\n",
       " 0.6782597638920624,\n",
       " 0.659060099133319,\n",
       " 0.5987475523670053,\n",
       " 0.6440802798031156,\n",
       " 0.259304542969238,\n",
       " 0.007122277609127647,\n",
       " 0.5947567701201621,\n",
       " 0.34748315690253595,\n",
       " 0.9999895556027665,\n",
       " 0.6125993913440019,\n",
       " 0.5828311700714786,\n",
       " 0.9907440215476224,\n",
       " 0.5709226141257464,\n",
       " 0.18504341771442367,\n",
       " 0.8687767799818036,\n",
       " 0.6347299194867233,\n",
       " 0.9918330700119063,\n",
       " 0.6332045730714689,\n",
       " 0.037788696237097796,\n",
       " 0.9873690252325346,\n",
       " 0.6245157294518607,\n",
       " 0.5153807136953198,\n",
       " 0.9740097037573644,\n",
       " 0.06195916713930416,\n",
       " 0.5817280845062422,\n",
       " 0.7019603628938752,\n",
       " 0.9317881256667304,\n",
       " 0.8094208677118544,\n",
       " 0.2581464675381854,\n",
       " 0.5436571727435006,\n",
       " 0.8283407774939829,\n",
       " 0.8902350801990154,\n",
       " 0.7348918635836464,\n",
       " 0.04349306928692498,\n",
       " 0.930371289096828,\n",
       " 0.5208802619359836,\n",
       " 0.6287314215804835,\n",
       " 0.13000483223892967,\n",
       " 0.6685736128614708,\n",
       " 0.016453051659012785,\n",
       " 0.06254288609598513,\n",
       " 0.47870209635847927]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import  BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def request_url(url):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent} \n",
    "    \n",
    "    res = requests.get(url,headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    return res.text\n",
    "\n",
    "url = 'https://finance.sina.com.cn/roll/index.d.html?cid=56588&page=1'\n",
    "soup = BeautifulSoup(request_url(url), 'lxml')\n",
    "\n",
    "info = [inf.text for inf in soup.find_all('a', target = '_blank')][2:]\n",
    "\n",
    "senti = [SnowNLP(text).sentiments for text in info]\n",
    "senti \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情感评分（0.6以上为积极，0.2一下为负面）： 0.58\n"
     ]
    }
   ],
   "source": [
    "print(\"情感评分（0.6以上为积极，0.2一下为负面）：\",np.round(np.mean(senti),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
